{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d2c6c425",
   "metadata": {},
   "source": [
    "cifar10\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd307b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5031c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the data in train and test\n",
    "# x is indepenedent variable\n",
    "# y is dependent variable\n",
    "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
    "# load data is use fo import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d1e329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing data types\n",
    "x_train=x_train.astype(\"float32\")/255.0\n",
    "x_test=x_test.astype(\"float32\")/255.0\n",
    "# here we divide with the 255.0 for making the value in b/w [0to1]\n",
    "# astype we changed into float32 because on train model we usally gets the value in float type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2492dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating layers in the models\n",
    "model= keras.Sequential(\n",
    "     [\n",
    "         keras.Input(shape=(32,32,3)),\n",
    "         #            hight,wigth,color-channels\n",
    "         layers.Conv2D(32, (3,3), padding='valid'),\n",
    "         #  output-channel,kernal-size\n",
    "         layers.MaxPooling2D(pool_size=(2,2)),\n",
    "         layers.Conv2D(64,3,activation='relu'),\n",
    "         layers.MaxPooling2D(),\n",
    "         layers.Conv2D(128,3,activation='relu'),\n",
    "         layers.Flatten(),\n",
    "         layers.Dense(64, activation='relu'),\n",
    "         layers.Dense(10),\n",
    "         # output layers\n",
    "         \n",
    "         \n",
    "     ]\n",
    ")\n",
    "def my_model():\n",
    "    input=keras.Input(shape=(32,32,3))\n",
    "    \n",
    "    x=layers.Conv2D(32,3)(inputs)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    x=layers.activations.relu(x)\n",
    "    x=layers.MaxPooling2d()(x)\n",
    "    x=layers.Conv2D(64,5, padding='same')(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    x=layers.activation.relu(x)\n",
    "    x=layers.Conv2D(128,3)(x)\n",
    "    x=keras.activation.relu(x)\n",
    "    x=layers.flatten()(x)\n",
    "    x=layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    outputs= layers.Dense(10)(x)\n",
    "    \n",
    "    model= keras.Model(input=input, output=outputs)\n",
    "                      \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d650246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "08f83799",
   "metadata": {},
   "source": [
    "## ‘softmax’.\n",
    "\n",
    "\n",
    "Flatten serves as a connection between the convolution and dense layers\n",
    "Dense' is the layer type we will use in for our output layer."
   ]
  },
  {
   "cell_type": "raw",
   "id": "10f7d5df",
   "metadata": {},
   "source": [
    "Trainable parameters are the number of, well, trainable elements in your network;\n",
    "neurons that are affected by backpropagation. \n",
    "For example, for the Wx + b operation in each neuron, W and b are trainable – because they are changed by optimizers after backpropagation was applied for gradient computation. Nontrainable ones are \n",
    "e.g. Batch Normalization or when you lock/freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51d446a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilation model\n",
    "#  Compiling the model takes three parameters: optimizer, loss and metrics.\n",
    "model.compile(\n",
    "     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "     optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "     metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4188bacb",
   "metadata": {},
   "source": [
    "validation data:\n",
    "we will use the test set provided to us in our dataset, which we have split into X_test and y_test.\n",
    "\n",
    "epochs:\n",
    "The number of epochs is the number of times the model will cycle through the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18abcb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 - 60s - loss: 0.7087 - accuracy: 0.7568 - 60s/epoch - 77ms/step\n",
      "Epoch 2/3\n",
      "782/782 - 57s - loss: 0.6798 - accuracy: 0.7644 - 57s/epoch - 72ms/step\n",
      "Epoch 3/3\n",
      "782/782 - 57s - loss: 0.6441 - accuracy: 0.7777 - 57s/epoch - 73ms/step\n",
      "157/157 - 4s - loss: 0.8193 - accuracy: 0.7203 - 4s/epoch - 23ms/step\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_11 (Conv2D)          (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                131136    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(x_train, y_train, batch_size=64,epochs=3,verbose=2 )\n",
    "model.evaluate(x_test, y_test, batch_size=64, verbose=2)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f2bfb04",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent, or SGD for short, is an optimization algorithm used to train machine learning algorithms, most notably artificial neural networks used in deep learning.\n",
    "\n",
    "What Is a Batch?\n",
    "The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "\n",
    "The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
